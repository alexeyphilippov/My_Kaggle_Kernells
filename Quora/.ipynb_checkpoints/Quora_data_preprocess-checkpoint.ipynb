{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksejfilippov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksejfilippov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets shape: (1306122, 3)\n",
      "Test datasets shape: (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/aleksejfilippov/Desktop/techotrack/quora_2/csvs/train.csv')\n",
    "test = pd.read_csv('/Users/aleksejfilippov/Desktop/techotrack/quora_2/csvs/test.csv')\n",
    "print(\"Train datasets shape:\", train.shape)\n",
    "print(\"Test datasets shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Throw away most common words and all signs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = ['what','when','why','which','who','how', 'whose', 'whome', 'people', 'i', \n",
    "                  'n\\'t','\\'s','like','get','would','would','many', 'want', 'good', 'india', 'girl',\n",
    "                  'first', 'take', 'much', 'ever', 'take', 'feel', 'know', 'think', 'make', \n",
    "                  'year', 'time', 'still', 'life', 'country', 'world', 'question', 'even', 'really',\n",
    "                  'love', 'better', 'human', 'right', 'thing', 'could', 'give', 'person', 'child']\n",
    "stop_signs = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "stop_words = stopwords.words('english')\n",
    "for w in frequent_words+stop_signs:\n",
    "    stop_words.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Words lemmatization, using only words with more than 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_questions_train = []\n",
    " \n",
    "for sentence in train['question_text']:\n",
    "    new_sentence = [wordnet_lemmatizer.lemmatize(w).lower() for w in word_tokenize(sentence)]\n",
    "    new_sentence = [w for w in new_sentence if w not in stop_words]\n",
    "    new_sentence = [w for w in new_sentence if len(w)>3]\n",
    "         \n",
    "    clean = ' '.join(new_sentence)    \n",
    "    if len(clean) == 0: \n",
    "        cleaned_questions_train.append('0')\n",
    "    else:\n",
    "        cleaned_questions_train.append(clean)\n",
    "\n",
    "cleaned_questions_test = []\n",
    "for sentence in test['question_text']:\n",
    "    new_sentence = [wordnet_lemmatizer.lemmatize(w).lower() for w in word_tokenize(sentence)]\n",
    "    new_sentence = [w for w in new_sentence if w not in stop_words]\n",
    "    new_sentence = [w for w in new_sentence if len(w)>3]\n",
    "       \n",
    "    clean = ' '.join(new_sentence)    \n",
    "    if len(clean) == 0:\n",
    "        cleaned_questions_test.append('0')\n",
    "    else:\n",
    "        cleaned_questions_test.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.concat([train.qid, train.target], axis = 1, copy = True)\n",
    "test1 = pd.concat([test.qid], axis = 1, copy = True)\n",
    "train1.insert(loc = 0, column=\"debugged_questions\", value=cleaned_questions_train)\n",
    "test1.insert(loc = 0, column=\"debugged_questions\", value=cleaned_questions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 TF-IDF. Using feature selection to find 500 most meanigful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tfv.fit(train1.debugged_questions.tolist() + test1.debugged_questions.tolist())\n",
    "\n",
    "train2_ne =  tfv.transform(train1.debugged_questions)\n",
    "test2_ne =  tfv.transform(test1.debugged_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectKBest(chi2, k=500)\n",
    "X_new = select.fit_transform(train2_ne, train1.target)\n",
    "names = tfv.get_feature_names()\n",
    "selected_words = np.asarray(names)[select.get_support()]\n",
    "print('Here goes a list of most meaningful words and word combinations of length less than four:\\n')\n",
    "print(', '.join(selected_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.030734062194824\n"
     ]
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "tr_idf = pd.DataFrame(train2_ne[:, select.get_support()].toarray(), dtype = 'float16')\n",
    "te_idf = pd.DataFrame(test2_ne.T[select.get_support()].T.toarray(), dtype = 'float16')\n",
    "\n",
    "tr_idf.columns = ['idf' + str(tok) for tok in np.arange(500)]\n",
    "te_idf.columns = ['idf' + str(tok) for tok in np.arange(500)]\n",
    "\n",
    "train2_idf = pd.concat([tr_idf, train1.qid, train1.target], axis = 1)\n",
    "test2_idf = pd.concat([te_idf, test1.qid], axis = 1)\n",
    "print(time.time() - start1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Glove.840B.300d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload embeddings even if already loaded (for each section to be independant)\n",
    "EMBEDDING_FILE = '/Users/aleksejfilippov/Desktop/techotrack/quora_2/Data_preparation/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_of_vectors = []\n",
    "for sentence in train1.debugged_questions.tolist():\n",
    "    splitted = sentence.split(' ')\n",
    "    vector = np.zeros(300).astype(np.float16)\n",
    "    for token in splitted:\n",
    "        try:\n",
    "            vector = vector + embeddings_index[token]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    train_list_of_vectors.append(vector.astype(np.float16))\n",
    "    \n",
    "test_list_of_vectors = []\n",
    "for sentence in test1.debugged_questions.tolist():\n",
    "    splitted = sentence.split(' ')\n",
    "    vector = np.zeros(300).astype(np.float16)\n",
    "    for token in splitted:\n",
    "        try:\n",
    "            vector = vector + embeddings_index[token]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    test_list_of_vectors.append(vector.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = time.time()\n",
    "tr_glove = pd.DataFrame(train_list_of_vectors, dtype = 'float16')\n",
    "te_glove = pd.DataFrame(test_list_of_vectors, dtype = 'float16')\n",
    "\n",
    "tr_glove.columns = ['glove'+str(tok) for tok in np.arange(300)]\n",
    "tr_glove.columns = ['glove'+str(tok) for tok in np.arange(300)]\n",
    "\n",
    "train2_glove = pd.concat([tr_glove, train1.qid, train1.target], axis = 1)\n",
    "test2_glove = pd.concat([te_glove, test1.qid], axis = 1)\n",
    "print(time.time() - start1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Latent Direchlet Allocation model to be used as extra features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [] #trtok\n",
    "for sentence in train1.debugged_questions.tolist():\n",
    "    train_tokens.append(sentence.split())\n",
    "test_tokens = []\n",
    "for sentence in test1.debugged_questions.tolist():\n",
    "    test_tokens.append(sentence.split())\n",
    "    \n",
    "all_tokens = train_tokens + test_tokens\n",
    "\n",
    "dictionary = corpora.Dictionary(all_tokens)  \n",
    "corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "    \n",
    "test_corpora = [] #tecor\n",
    "train_corpora = [] #trcor\n",
    "\n",
    "for token in test_tokens: test_corpora.append(dictionary.doc2bow(token))\n",
    "for token in train_tokens: train_corpora.append(dictionary.doc2bow(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=8, passes=5)\n",
    "# ldamodel = models.ldamodel.LdaModel.load(\"/Users/aleksejfilippov/Desktop/techotrack/quora/ldamodel3_lkcd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create matrix Theta describing the destribution of topics above documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topics = ldamodel.get_document_topics(test_corpora)\n",
    "train_topics = ldamodel.get_document_topics(train_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents_topics = [] #nums1\n",
    "for list_of_topics in [ldamodel.get_document_topics(corp) for corp in train_corpora]:\n",
    "    i = i + 1\n",
    "    document_topics = [] #num1\n",
    "    for tup in list_of_topics:\n",
    "        document_topics.append(tup)\n",
    "    train_documents_topics.append(document_topics)\n",
    "\n",
    "test_documents_topics = []\n",
    "for list_of_topics in [ldamodel.get_document_topics(corp) for corp in test_corpora]:\n",
    "    document_topics = []\n",
    "    for tup in list_of_topics:\n",
    "        document_topics.append(tup)\n",
    "    test_documents_topics.append(document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ps_tr = np.ndarray([len(train_corpora), 8])\n",
    "X_ps_te = np.ndarray([len(test_corpora), 8])\n",
    "for i in range(len(train_documents_topics)):\n",
    "    for j in range(len(train_documents_topics[i])):\n",
    "        X_ps_tr[i, int(train_documents_topics[i][j][0])] = train_documents_topics[i][j][1]\n",
    "        \n",
    "        \n",
    "for i in range(len(test_documents_topics)):\n",
    "    for j in range(len(test_documents_topics[i])):\n",
    "        X_ps_te[i, test_documents_topics[i][j][0]] = test_documents_topics[i][j][1]\n",
    "        \n",
    "X_ps_te = pd.DataFrame(X_ps_te, dtype = 'float16')\n",
    "X_ps_tr = pd.DataFrame(X_ps_tr, dtype = 'float16')\n",
    "\n",
    "X_ps_tr.columns = ['lda'+str(tok) for tok in np.arange(8)]\n",
    "X_ps_te.columns = ['lda'+str(tok) for tok in np.arange(8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Concat Theta as new feature-matrix to existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_idf_lda = pd.concat([X_ps_tr, train2_idf], axis = 1)\n",
    "test2_idf_lda = pd.concat([X_ps_te, test2_idf], axis = 1)\n",
    "\n",
    "train2_glove_lda = pd.concat([X_ps_tr, train2_glove], axis = 1)\n",
    "test2_glove_lda = pd.concat([X_ps_te, test2_glove], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_idf_lda.columns = ['lda'+str(tok) for tok in np.arange(8)] + ['idf' + str(tok) for tok in np.arange(500)] + ['qid', 'target']\n",
    "test2_idf_lda.columns = ['lda'+str(tok) for tok in np.arange(8)] + ['idf' + str(tok) for tok in np.arange(500)] + ['qid']\n",
    "\n",
    "train2_glove_lda.columns = ['lda'+str(tok) for tok in np.arange(8)] + ['glove' + str(tok) for tok in np.arange(300)] + ['qid', 'target']\n",
    "test2_glove_lda.columns = ['lda'+str(tok) for tok in np.arange(8)] + ['glove' + str(tok) for tok in np.arange(300)] + ['qid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glove0</th>\n",
       "      <th>glove1</th>\n",
       "      <th>glove2</th>\n",
       "      <th>glove3</th>\n",
       "      <th>glove4</th>\n",
       "      <th>glove5</th>\n",
       "      <th>glove6</th>\n",
       "      <th>glove7</th>\n",
       "      <th>glove8</th>\n",
       "      <th>glove9</th>\n",
       "      <th>...</th>\n",
       "      <th>qid</th>\n",
       "      <th>target</th>\n",
       "      <th>lda0</th>\n",
       "      <th>lda1</th>\n",
       "      <th>lda2</th>\n",
       "      <th>lda3</th>\n",
       "      <th>lda4</th>\n",
       "      <th>lda5</th>\n",
       "      <th>lda6</th>\n",
       "      <th>lda7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.632324</td>\n",
       "      <td>-0.392578</td>\n",
       "      <td>1.703125</td>\n",
       "      <td>-0.258301</td>\n",
       "      <td>2.683594</td>\n",
       "      <td>-0.836914</td>\n",
       "      <td>-0.585449</td>\n",
       "      <td>1.135742</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>10.109375</td>\n",
       "      <td>...</td>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187506</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.520843</td>\n",
       "      <td>0.187484</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.660645</td>\n",
       "      <td>-0.067566</td>\n",
       "      <td>-0.893066</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>-0.841797</td>\n",
       "      <td>0.852539</td>\n",
       "      <td>-0.271484</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>9.406250</td>\n",
       "      <td>...</td>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.625005</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.224995</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.811035</td>\n",
       "      <td>3.775391</td>\n",
       "      <td>-2.605469</td>\n",
       "      <td>0.283936</td>\n",
       "      <td>-2.291016</td>\n",
       "      <td>1.155273</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>1.822266</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>7.058594</td>\n",
       "      <td>...</td>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017885</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.446426</td>\n",
       "      <td>0.446403</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198120</td>\n",
       "      <td>0.214722</td>\n",
       "      <td>-0.312744</td>\n",
       "      <td>0.356445</td>\n",
       "      <td>0.397949</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>-0.939453</td>\n",
       "      <td>0.811523</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>0.467773</td>\n",
       "      <td>...</td>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.854052</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.020852</td>\n",
       "      <td>0.020865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.681641</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>-1.675781</td>\n",
       "      <td>-1.631836</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>1.447266</td>\n",
       "      <td>0.417725</td>\n",
       "      <td>0.409424</td>\n",
       "      <td>-4.195312</td>\n",
       "      <td>3.080078</td>\n",
       "      <td>...</td>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386566</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>0.248629</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>0.015650</td>\n",
       "      <td>0.286622</td>\n",
       "      <td>0.015634</td>\n",
       "      <td>0.015633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     glove0    glove1    glove2    glove3    glove4    glove5    glove6  \\\n",
       "0  0.632324 -0.392578  1.703125 -0.258301  2.683594 -0.836914 -0.585449   \n",
       "1  0.660645 -0.067566 -0.893066 -0.076050  0.353516 -0.841797  0.852539   \n",
       "2  0.811035  3.775391 -2.605469  0.283936 -2.291016  1.155273  0.001010   \n",
       "3  0.198120  0.214722 -0.312744  0.356445  0.397949  0.316162 -0.939453   \n",
       "4  3.681641  1.371094 -1.675781 -1.631836 -0.024017  1.447266  0.417725   \n",
       "\n",
       "     glove7    glove8     glove9  ...                   qid  target      lda0  \\\n",
       "0  1.135742  0.002935  10.109375  ...  00002165364db923c7e6       0  0.187506   \n",
       "1 -0.271484  0.011772   9.406250  ...  000032939017120e6e44       0  0.025000   \n",
       "2  1.822266  0.608887   7.058594  ...  0000412ca6e4628ce2cf       0  0.017857   \n",
       "3  0.811523 -0.875000   0.467773  ...  000042bf85aa498cd78e       0  0.020847   \n",
       "4  0.409424 -4.195312   3.080078  ...  0000455dfa3e01eae3af       0  0.386566   \n",
       "\n",
       "       lda1      lda2      lda3      lda4      lda5      lda6      lda7  \n",
       "0  0.020833  0.520843  0.187484  0.020833  0.020833  0.020833  0.020833  \n",
       "1  0.025000  0.625005  0.025000  0.025000  0.224995  0.025000  0.025000  \n",
       "2  0.017885  0.017857  0.446426  0.446403  0.017857  0.017857  0.017857  \n",
       "3  0.020845  0.020847  0.020846  0.854052  0.020847  0.020852  0.020865  \n",
       "4  0.015633  0.248629  0.015633  0.015650  0.286622  0.015634  0.015633  \n",
       "\n",
       "[5 rows x 310 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2_glove_lda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_idf_lda.to_csv('train2_idf_lda.csv')\n",
    "test2_idf_lda.to_csv('test2_idf_lda.csv')\n",
    "\n",
    "train2_glove_lda.to_csv('train2_glove_lda.csv')\n",
    "test2_glove_lda.to_csv('test2_glove_lda.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of data preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
